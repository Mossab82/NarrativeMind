training:
  batch_size: 16
  learning_rate: 2e-5
  warmup_steps: 500
  max_epochs: 5
  early_stopping:
    patience: 2
    min_delta: 0.001
  optimizer:
    type: "AdamW"
    weight_decay: 0.01
    
model:
  base_model: "asafaya/bert-base-arabic"
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  vocab_size: 64000
  
generation:
  max_length: 512
  num_beams: 5
  temperature: 0.7
  no_repeat_ngram_size: 3
  do_sample: true
  
evaluation:
  metrics:
    - "bleu"
    - "cultural_preservation"
    - "dialectal_accuracy"
    - "coherence"
  human_evaluation:
    num_annotators: 50
    min_agreement: 0.7
    
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  dialects:
    - "egyptian"
    - "gulf"
    - "levantine"
    - "moroccan"
    - "msa"
