training:
  batch_size: 16
  learning_rate: 2e-5
  warmup_steps: 500
  max_epochs: 5
  early_stopping:
    patience: 2
    min_delta: 0.001
  optimizer:
    type: "AdamW"
    weight_decay: 0.01
    
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  
logging:
  log_every_n_steps: 100
  save_every_n_epochs: 1
